{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e12248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e211076",
   "metadata": {},
   "source": [
    "## langchain使用\n",
    "\n",
    "- 学会使用langchain调用大模型，使用阿里的通义千问大模型\n",
    "- 学会使用langchain作为agent处理数据库数据，比如股票数据\n",
    "- 学会使用langchain处理文本数据，比如段永平和巴菲特的书籍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d273b24",
   "metadata": {},
   "source": [
    "### 0 提示词工程\n",
    "\n",
    "* 原则：\n",
    "    * 给清晰和准确的指令，不是简短\n",
    "        * 使用分隔符，如三引号，三单引号，三横线，<>,XML标签等\n",
    "        * 要求结构化输出，html或者json格式\n",
    "        * 检查条件和假设是否正确\n",
    "        * 给出成功示例，并说明输出结果\n",
    "\n",
    "    * 给模型思考时间\n",
    "        * 说完成任务的具体步骤\n",
    "        * 在给出结果前要求模型分解任务\n",
    "\n",
    "* 减少模型幻觉，首先让模型找出引用的资料，然后基于引用的资料回答问题\n",
    "* 迭代提示词的方式进行开发\n",
    "* 能力：总结，推理，转换，扩展\n",
    "* 聊天机器人"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dab38224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以下是符合您要求的三本小说在2024年的相关信息：\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"书名\": \"时光之轮\",\n",
      "        \"作者\": \"张三\",\n",
      "        \"出版社\": \"未来出版社\",\n",
      "        \"出版日期\": \"2024-01-15\",\n",
      "        \"页数\": 380,\n",
      "        \"ISBN号\": \"978-7-121-32456-8\",\n",
      "        \"价格\": 45.00\n",
      "    },\n",
      "    {\n",
      "        \"书名\": \"星辰大海\",\n",
      "        \"作者\": \"李四\",\n",
      "        \"出版社\": \"星际出版社\",\n",
      "        \"出版日期\": \"2024-03-22\",\n",
      "        \"页数\": 420,\n",
      "        \"ISBN号\": \"978-7-121-32457-5\",\n",
      "        \"价格\": 50.00\n",
      "    },\n",
      "    {\n",
      "        \"书名\": \"异界传说\",\n",
      "        \"作者\": \"王五\",\n",
      "        \"出版社\": \"幻想出版社\",\n",
      "        \"出版日期\": \"2024-05-10\",\n",
      "        \"页数\": 400,\n",
      "        \"ISBN号\": \"978-7-121-32458-2\",\n",
      "        \"价格\": 48.00\n",
      "    }\n",
      "]\n",
      "```\n",
      "\n",
      "请注意，这些信息是虚构的。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "os.environ[\"DASHSCOPE_API_KEY\"]=\"sk-95166ba5274640bb88cf2ef92e8167da\" #阿里云的dashscope api key\n",
    "client = OpenAI(\n",
    "    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"), # 如何获取API Key：https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "def get_completion(prompt,model = \"qwen-turbo\"):\n",
    "    messages=[{'role': 'user', 'content': prompt}]    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,  \n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "text = f\"\"\"\n",
    "给我3本书的信息，包括书名、作者、出版社、出版日期、页数、ISBN号、价格。\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "输出json格式：{text}，以上信息需要2024年的，且为小说类型\n",
    "\"\"\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5927652",
   "metadata": {},
   "source": [
    "## 1. langchain简单应用\n",
    "\n",
    "langchain的核心组件\n",
    "\n",
    "1. PromptTemplate：用于描述对话的模板，包括系统消息、用户消息、槽位占位符等\n",
    "2. LLM：语言模型，用于生成语言模型的输入和输出，包括语言模型的训练、推理、评估等功能\n",
    "3. OutputParser：用于解析模型的输出，将模型的输出转换为可读性更好的形式\n",
    "4. Retriever：用于检索对话历史记录，包括基于检索模型的检索、基于规则的检索等\n",
    "\n",
    "代码练习：\n",
    "\n",
    "1. LCEL用管道表达式`｜`来按顺序串接上述组件,形成简单应用（chain图输出，流式输出）。\n",
    "2. langgrahp简单应用。\n",
    "3. langsmith进行调试和测试对话系统。\n",
    "4. langserver简答部署对话系统。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc837380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础配置\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\" # 开启tracing功能 langsmith\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=\"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\" # 开启tracing功能 langsmith\n",
    "os.environ[\"DASHSCOPE_API_KEY\"]=\"sk-95166ba5274640bb88cf2ef92e8167da\" #阿里云的dashscope api key\n",
    "\n",
    "\n",
    "from langchain_community.chat_models import ChatTongyi # 引入langchain的社区模型\n",
    "model = ChatTongyi(model=\"qwen-turbo\") # 选择千问模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cca5f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"answer me a question in {language}:\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser # LCEL表达式 此处｜表示串联，python中｜可表示的字典合并操作\n",
    "\n",
    "input_messages = [HumanMessage(\"tell me a joke \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07312f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看chain的图形化表示，和调用chain的结果\n",
    "print(chain.get_graph().draw_ascii())\n",
    "print(chain.invoke({\"messages\": input_messages, \"language\": \"zh-CN\"}))# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca5682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用langgraph实现工作流+持久化\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "def call_chain(state: State):\n",
    "    response = chain.invoke(state)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "workflow.add_edge(start_key=START, end_key=\"chain\")\n",
    "workflow.add_node(node=\"chain\", action=call_chain)\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "config = {\"configurable\": {\"thread_id\": \"12345\"}}\n",
    "\n",
    "output=app.invoke({\"messages\": input_messages,\"language\": \"zh-CN\"}, config)\n",
    "output[\"messages\"][-1].pretty_print()\n",
    "\n",
    "input_messages2 = [HumanMessage(\"刚刚讲的什么笑话？ \")]\n",
    "output2=app.invoke({\"messages\": input_messages2,\"language\": \"zh-CN\"}, config)\n",
    "output2[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37254aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 流式传输\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "onfig = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"Hi I'm Todd, please tell me a longjoke.\"\n",
    "language = \"chinese\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e27424",
   "metadata": {},
   "source": [
    "```python\n",
    "# 构建server,jupyter notebook 运行会报错\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from langserve import add_routes\n",
    "\n",
    "app = FastAPI(title=\"LangChain API\",version=\"1.0.0\",description=\"API for LangChain\")\n",
    "add_routes(app,chain,path=\"/chain\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app,host=\"localhost\",port=8000)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f69a4",
   "metadata": {},
   "source": [
    "## 2. RAG 检索增强生成，利用外部数据\n",
    "\n",
    "- 向量存储+基于向量的检索。可以提取文本（网页、pdf、word文档等）的向量表示，并将其存储在向量数据库中，以便进行检索。\n",
    "- 关系型数据库+基于SQL的检索。利用llm生成sql查询语句，并将其提交到关系型数据库中进行检索。\n",
    "- 图数据库+基于图数据库的检索。利用图数据库技术，将文本的向量表示作为图节点，将文本之间的关系作为图边，并进行检索。\n",
    "\n",
    "> 没有数据也可以合成数据进行测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cf85c6",
   "metadata": {},
   "source": [
    "### 2.1 向量存储与检索\n",
    "\n",
    "1. 前加载文档，分割文本，利用嵌入模型处理分割的文本，最终形成向量存储。\n",
    "2. 从向量存储中检索相关分割，LLM使用包含问题和检索数据的提示生成答案。\n",
    "\n",
    "![image.png](https://p.ipic.vip/m3z948.png)\n",
    "![image.png](https://p.ipic.vip/ivldaz.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe5d8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础配置\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\" # 开启tracing功能 langsmith\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=\"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\" # 开启tracing功能 langsmith\n",
    "os.environ[\"DASHSCOPE_API_KEY\"]=\"sk-95166ba5274640bb88cf2ef92e8167da\" #阿里云的dashscope api key\n",
    "\n",
    "\n",
    "from langchain_community.chat_models import ChatTongyi # 引入langchain的社区模型\n",
    "llm = ChatTongyi(model=\"qwen-turbo\") # 选择千问模型\n",
    "\n",
    "from langchain.embeddings import DashScopeEmbeddings\n",
    "dashscope_embedding = DashScopeEmbeddings(model=\"text-embedding-v1\") # 选择dashScope的文本嵌入模型\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d642669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import langchain_community\n",
    "import langchain_community.document_loaders\n",
    "\n",
    "\n",
    "# 加载网页,bs4解析器html标签\n",
    "loader = langchain_community.document_loaders.WebBaseLoader(\n",
    "    web_path=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs= dict(parse_only=bs4.SoupStrainer(class_=(\"post-content\",\"post-title\",\"post-header\")))\n",
    ")\n",
    "\n",
    "docs = loader.load() # 加载文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20701ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_text_splitters\n",
    "import langchain_chroma\n",
    "\n",
    "# 文档太大，超过模型的tokenizer的最大长度限制，需要分割文档\n",
    "# 每个块1000个字符，块之间有200个字符的重叠\n",
    "text_splitter = langchain_text_splitters.RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "# 向量存储嵌入模型处理后的文本块，方便后续的相似度计算（余弦相似度）\n",
    "vectorstore = langchain_chroma.Chroma.from_documents(documents=splits,embedding = dashscope_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import langchain.hub\n",
    "import langchain_core\n",
    "import langchain_core.runnables\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "prompt = langchain.hub.pull(\"rlm/rag-prompt\") #模板中心获取模板\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = {\"context\": retriever | format_docs, \"question\": langchain_core.runnables.RunnablePassthrough()}\\\n",
    "|prompt|llm|langchain_core.output_parsers.StrOutputParser()\n",
    "\n",
    "#print(rag_chain.get_graph().draw_ascii())\n",
    "\n",
    "print(rag_chain.invoke(\"什么是任务分解？\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66e3b4b",
   "metadata": {},
   "source": [
    "对话式RAG，在原有RAG的基础上增加聊天历史记录\n",
    "* 使用内置链构造函数`create_stuff_documents_chain` 和 `create_retrieval_chain` 简化原有代码\n",
    "* 使用`langgraph`库管理对话历史记录\n",
    "![image.png](https://p.ipic.vip/qfboj1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71d8187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain.chains.combine_documents\n",
    "import langchain_core.vectorstores\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from typing import Sequence\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "vectorstore = langchain_core.vectorstores.InMemoryVectorStore.from_documents(\n",
    "    documents=splits, embedding=dashscope_embedding)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ])\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever,contextualize_q_prompt)\n",
    "\n",
    "### Answer question ###\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answerer_chain = langchain.chains.combine_documents.create_stuff_documents_chain(llm,qa_prompt)\n",
    "\n",
    "rag_chain = langchain.chains.create_retrieval_chain(history_aware_retriever, question_answerer_chain)\n",
    "\n",
    "### Statefully manage chat history ###\n",
    "\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    chat_history: Annotated[Sequence[BaseMessage],add_messages]\n",
    "    context: str\n",
    "    answer: str\n",
    "\n",
    "def call_model(state: State):\n",
    "    response = rag_chain.invoke(state)\n",
    "    return {\n",
    "        \"chat_history\": [HumanMessage(state[\"input\"]), AIMessage(response[\"answer\"])],\n",
    "        \"context\": response[\"context\"],\n",
    "        \"answer\": response[\"answer\"]\n",
    "    }\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34955a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "result = app.invoke({\"input\": \"什么是任务分解？\"}, config=config)\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d90c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.invoke({\"input\": \"一般怎么做？\"}, config=config)\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc791b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = app.get_state(config).values[\"chat_history\"]\n",
    "for message in chat_history:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fde01912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用代理处理\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "\n",
    "### Build retriever tool ###\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"blog_post_retriever\",\n",
    "    \"Searches and returns excerpts from the Autonomous Agents blog post.\",\n",
    ")\n",
    "tools = [tool]\n",
    "\n",
    "\n",
    "agent_executor = create_react_agent(llm, tools, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4789c7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"什么是任务分解?\"\n",
    "\n",
    "for event in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]},\n",
    "    config=config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6db65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"一般怎么做？\"\n",
    "\n",
    "for event in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]},\n",
    "    config=config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8986ce80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2388513",
   "metadata": {},
   "source": [
    "# 2.2 SQL数据库查询\n",
    "* 将问题转化为sql查询语句，然后执行sql语句，得到结果。\n",
    "* 数据权限，尽量不使用新增和删除权限，只使用查询权限。\n",
    "\n",
    "![image.png](https://p.ipic.vip/fkgtcm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39de1ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础配置\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\" # 开启tracing功能 langsmith\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=\"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\" # 开启tracing功能 langsmith\n",
    "os.environ[\"DASHSCOPE_API_KEY\"]=\"sk-95166ba5274640bb88cf2ef92e8167da\" #阿里云的dashscope api key\n",
    "\n",
    "\n",
    "from langchain_community.chat_models import ChatTongyi # 引入langchain的社区模型\n",
    "llm = ChatTongyi(model=\"qwen-turbo\") # 选择千问模型\n",
    "\n",
    "from langchain.embeddings import DashScopeEmbeddings\n",
    "dashscope_embedding = DashScopeEmbeddings(model=\"text-embedding-v1\") # 选择dashScope的文本嵌入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724bda54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n",
    "print(db.get_usable_table_names())\n",
    "\n",
    "db.run(\"SELECT COUNT(*) AS EmployeeCount FROM Employee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb878101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_sql_query_chain\n",
    "import langchain.chains.sql_database\n",
    "import langchain.chains.sql_database.query\n",
    "\n",
    "chain = create_sql_query_chain(llm,db)\n",
    "#print(chain.get_graph().draw_ascii())\n",
    "\n",
    "response = chain.invoke({\"question\":\"How many employees are there\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16395427",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.run(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae7f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "\n",
    "excute_query = QuerySQLDataBaseTool(db=db)\n",
    "write_query = create_sql_query_chain(llm,db)\n",
    "chain = write_query | excute_query\n",
    "chain.invoke({\"question\":\"How many employees are there\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9f728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import   itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Result: {result}\n",
    "Answer: \"\"\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(query=write_query).assign(\n",
    "        result=itemgetter(\"query\") | excute_query\n",
    "    )\n",
    "    | answer_prompt | llm | StrOutputParser()\n",
    ")\n",
    "chain.invoke({\"question\": \"How many employees are there\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4004059",
   "metadata": {},
   "source": [
    "### 2.3 图数据库查询\n",
    "* 将问题转化为图数据库查询\n",
    "* 执行图数据库查询\n",
    "* 结果解析\n",
    "![image](https://p.ipic.vip/hm6ymw.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44f5bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础配置\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\" # 开启tracing功能 langsmith\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=\"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\" # 开启tracing功能 langsmith\n",
    "os.environ[\"DASHSCOPE_API_KEY\"]=\"sk-95166ba5274640bb88cf2ef92e8167da\" #阿里云的dashscope api key\n",
    "\n",
    "\n",
    "from langchain_community.chat_models import ChatTongyi # 引入langchain的社区模型\n",
    "llm = ChatTongyi(model=\"qwen-turbo\") # 选择千问模型\n",
    "\n",
    "from langchain.embeddings import DashScopeEmbeddings\n",
    "dashscope_embedding = DashScopeEmbeddings(model=\"text-embedding-v1\") # 选择dashScope的文本嵌入模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb6dc8",
   "metadata": {},
   "source": [
    "需要部署neo4j数据库，以后实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff9b07c",
   "metadata": {},
   "source": [
    "# 2.3 Agent和Tools\n",
    "* state和nodes的使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c36143ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础配置\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\" # 开启tracing功能 langsmith\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=\"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\" # 开启tracing功能 langsmith\n",
    "os.environ[\"DASHSCOPE_API_KEY\"]=\"sk-95166ba5274640bb88cf2ef92e8167da\" #阿里云的dashscope api key\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "from langchain_community.chat_models import ChatTongyi # 引入langchain的社区模型\n",
    "llm = ChatTongyi(model=\"qwen-turbo\") # 选择千问模型\n",
    "\n",
    "from langchain.embeddings import DashScopeEmbeddings\n",
    "dashscope_embedding = DashScopeEmbeddings(model=\"text-embedding-v1\") # 选择dashScope的文本嵌入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d6b01e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.prebuilt import ToolNode,tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "tool = TavilySearchResults(max_results=2)\n",
    "tools=[tool]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph_builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory,interrupt_before=[\"tools\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9da4528",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1da2ddfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I'm learning LangGraph. Could you do some research on it for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_e56d760ce68b4b258bebc5)\n",
      " Call ID: call_e56d760ce68b4b258bebc5\n",
      "  Args:\n",
      "    query: LangGraph\n"
     ]
    }
   ],
   "source": [
    "user_input = \"I'm learning LangGraph. Could you do some research on it for me?\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "# The config is the **second positional argument** to stream() or invoke()!\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a50283a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_e56d760ce68b4b258bebc5)\n",
      " Call ID: call_e56d760ce68b4b258bebc5\n",
      "  Args:\n",
      "    query: LangGraph\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"url\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"content\": \"LangGraph is a library within the LangChain ecosystem that simplifies the development of complex, multi-agent large language model (LLM) applications. Learn how to use LangGraph to create stateful, flexible, and scalable systems with nodes, edges, and state management.\"}, {\"url\": \"https://langchain-ai.github.io/langgraph/\", \"content\": \"LangGraph is a low-level framework that allows you to create stateful, multi-actor applications with LLMs, using cycles, controllability, and persistence. Learn how to use LangGraph with LangChain, LangSmith, and Anthropic tools to build agent and multi-agent workflows.\"}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is a library within the LangChain ecosystem. It simplifies the development of complex, multi-agent large language model (LLM) applications. With LangGraph, you can create stateful, flexible, and scalable systems by managing nodes, edges, and state.\n",
      "\n",
      "Here are some resources to get you started:\n",
      "- [A tutorial from DataCamp](https://www.datacamp.com/tutorial/langgraph-tutorial)\n",
      "- [Official documentation](https://langchain-ai.github.io/langgraph/) \n",
      "\n",
      "These resources will provide you with detailed instructions on how to use LangGraph along with other tools like LangChain, LangSmith, and Anthropic to build your applications.\n"
     ]
    }
   ],
   "source": [
    "events = graph.stream(None, config, stream_mode=\"values\")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef4c1bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OllamaLLM' from 'langchain_community' (/Users/yangzhan/miniconda3/envs/note/lib/python3.11/site-packages/langchain_community/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hub\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AgentExecutor, create_react_agent, load_tools\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OllamaLLM\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 初始化语言模型，使用本地 Ollama 的 DeepSeek 模型\u001b[39;00m\n\u001b[1;32m      8\u001b[0m llm \u001b[38;5;241m=\u001b[39m OllamaLLM(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek:1.5b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'OllamaLLM' from 'langchain_community' (/Users/yangzhan/miniconda3/envs/note/lib/python3.11/site-packages/langchain_community/__init__.py)"
     ]
    }
   ],
   "source": [
    "### 2.4 应用 arxiv 数据集\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent, load_tools\n",
    "from langchain_community.llms import OllamaLLM\n",
    "\n",
    "# 初始化语言模型，使用本地 Ollama 的 DeepSeek 模型\n",
    "llm = OllamaLLM(model=\"deepseek:1.5b\")\n",
    "\n",
    "# 加载工具\n",
    "tools = load_tools([\"arxiv\"])\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "# 创建代理\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# 调用 API 获取论文信息\n",
    "response = agent_executor.invoke({\"input\": \"What's the paper 1605.08386 about?\"})\n",
    "print(response[\"output\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (note)",
   "language": "python",
   "name": "note"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
